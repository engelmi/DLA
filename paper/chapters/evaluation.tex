\section{Evaluation}
\label{sec:evaluation}
For evaluation of the model that is described in section \ref{sec:model} the tool \textit{Tensorboard} is used. This is a suite of visualization tools that can be used to understand, debug and optimize \textit{TensorFlow} programs. In the context of evaluation, \textit{Tensorboard} is used to visualize the 
\begin{itemize}
	\item accuracy,
	\item loss and
	\item graph
\end{itemize}
of the model. As validation technique the k fold cross validation is being deployed. How this technique works and how it is integrated within the architecture is described in the subsection \ref{subsec:kfoldxvalidation}. In \ref{subsec:experiments} the yielded results of this technique are explained. Finally, the trained model and an unused dataset are used to predict the rising or falling of the contained stocks. In \ref{subsec:predresults} the outcome of this validation of the model against to this point unknown data to the model is presented. 

\subsection{K fold cross validation}
\label{subsec:kfoldxvalidation}
The most simple method for cross validation is the \textit{holdout} method. This technique splits the available dataset into two distinct sets - training and validation. In terms of variance this method is disadvantageous as it is not certain which data points end up in the validation set. This can lead to losing important patterns during training and, therefore, underfitting. 
\\
The technique of k fold cross validation provides a method of using the available data efficiently for training as well as validation. The data is split into $k$ subsets and the \textit{holdout} method is applied $k$ times to these sets. In each iteration a different set of the $k$ datasets is used as validation set and the remaining $k-1$ sets are used for training. As a result, each data point is used multiple times for training at some point in the loop and once for validation. Consequently, most of the data is used for fitting so that the bias is greatly reduced and by using most data for validation this also reduces the variance. The swap of training and validation data for each iteration also contributes to the efficiency of this technique. 
\\
The evaluation of the model by the k fold cross validation is achieved by logging the accuracy and loss. Those two scalars are measured for every iteration during the cross validation and after the $k$ iterations for the current epoch. During each of the $k$ iterations the mean of the accuracy and loss for all batches is calculated and written to a \textit{Tensorboard} chart. After the $k$ iterations the mean of all previously calculated average accuracies and losses is determined and also written to a \textit{Tensorboard} chart, whereas the x-axis displays the epoch of this result. The calculation for those mean values is done outside the \textit{TensorFlow} graph. Although this is not best practice, the mean calculation for those scalars would have been quite challenging to do inside the graph as a definition for the epoch is not possible at the level of graph definition. 

\subsection{Experiments for evaluation}
\label{subsec:experiments}
For the validation of the implementation experiments are executed using the \textit{SimpleLearningModel}, which is described in subsection \ref{subsec:simplelearningmodel}. Although the same model is being used for all experiments, the following parameters are varied for an optimal result:
\begin{itemize}
	\item dropout probability, 
	\item hidden size of the LSTM cell, 
	\item learning rate of the optimizer, 
	\item count of folds for cross validation $k$, 
	\item and the number of epochs
\end{itemize}
The mean accuracy and loss after each k fold cross validation as well as the mean overall accuracy and loss are being used as metrics for the rating of parameter configuration. These scalars are logged as described in the previous subsection \ref{subsec:kfoldxvalidation}. 
\\
Overall, two different settings of those parameters are executed. In table \ref{tbl:experiments} both configurations are depicted. 

TODO: nur dropout mal weglassen?!
Batch-Size: 20!!!!

\renewcommand{\arraystretch}{1.5}
\begin{table}[!ht]
	\begin{center}
		\begin{tabular}{c|c|c|c}
			 & \textbf{Experiment 1} & \textbf{Experiment 2} & \textbf{Experiment 3} \\
			\hline
			\textbf{Hidden size} & 100 &  100 & 200 \\
			\hline
			\textbf{Dropout probability} & 0.3 & 0.0 & 0.3\\
			\hline
			\textbf{Learning rate} & 1.0 & 1.0 & 2.0\\
			\hline
			\textbf{Number of folds k} & 10 & 10 & 10 \\
			\hline
			\textbf{Number of epochs} & 650 & 650 & 650 \\
		\end{tabular}
	\end{center}
	\caption{The configurations for both experiments. While the hidden size of the LSTM cell, the number of folds $k$ and the count of epochs remain the same for both experiments, the learning rate is halved as the dropout probability is being doubled. }
	\label{tbl:experiments}
\end{table}

Although the dropout probability should ... according to \cite{dropout}
\begin{itemize} 
	\item dropout (0.3) (wrapper)
	\item hidden size 100 (lstm cell)
	\item learning rate 1.0 (optimizer)
	- k of cross validation 10 (validation), 10 is empirical
	- num of epochs 650 
\end{itemize}

+ following setup
	
+ results can be seen in figures \ref{fig:acc650epochs} and \ref{fig:loss650epochs}
	
\begin{figure}[!ht]
	\caption{A Graph depicting the mean accuracy over 650 epochs of the k fold cross validation with $k=10$. Also, dropout was being used with a probability of 0.3. }
	\includegraphics[width=0.95\linewidth]{images/evaluation/650-epochs-k-crossvalidation-accuracy-mean.png}
	\label{fig:acc650epochs}
\end{figure}
\begin{figure}[!ht]
	\caption{A Graph depicting the mean loss over 650 epochs of the k fold cross validation with $k=10$. Also, dropout was being used with a probability of 0.3. }
	\includegraphics[width=0.95\linewidth]{images/evaluation/650-epochs-k-crossvalidation-loss-mean.png}
	\label{fig:loss650epochs}
\end{figure}


+ another experiment wit following setup
	- dropout (0.3) (wrapper)
	- hidden size 100 (lstm cell)
	- learning rate 0.5 (optimizer)
	- k of cross validation 10 (validation), 10 is empirical
	- num of epochs 1000


\subsection{OnlyStockLearningModel results}
\label{subsec:predresults}
In figure \ref{fig:OSLM_k-accuracy} you can see the accuracy of the three experiments described in table \ref{TODO}. The loss can be seen in figure \ref{fig:OSLM_k-loss}. The model without using dropout performed best. This seems to be reasonable because if the dropout drops the only input, like in the other two experiments, the model can not use any data for learning. The model with the biggest hidden size (experiment 3) has the highest loss but has a even better accuracy then the model of experiment 1. It would be interesting to see how this model performs without dropout.\\
\begin{figure}[tb]
	\caption{A Graph depicting the mean accuracy of the OnlyStockLearningModel over 650 epochs of the k fold cross validation with $k=10$. The blue line corresponds to experiment 2 and has the best accuracy with about $51\%$. The orange (experiment 1) and red (experiment 3) line have only an accuracy about $49\%$.}
	\includegraphics[width=0.95\linewidth]{images/OSLM_k-accuracy.PNG}
	\label{fig:OSLM_k-accuracy}
\end{figure}
\begin{figure}[tb]
	\caption{A Graph depicting the mean loss of the OnlyStockLearningModel over 650 epochs of the k fold cross validation with $k=10$. The red line corresponds to experiment 3 and has the highest loss with about $25$. The orange (experiment 1) and blue (experiment 2) line have nearly the same loss after 650 epochs. }
	\includegraphics[width=0.95\linewidth]{images/OSLM_k-loss.PNG}
	\label{fig:OSLM_k-loss}
\end{figure}
In the following listing you can see how the models performed on data not seen during training. The test dataset contains only 16 entries, so the results have to be watched carefully.
\begin{itemize}
	\item Experiment 1: $43.75\%$ accuracy (7 correct predictions)
	\item Experiment 2: $50\%$ accuracy (8 correct predictions)
	\item Experiment 3: $43.75\%$ accuracy (7 correct predictions)
\end{itemize}
It is interesting to see that the accuracy is always worse than during training. None of the models has a higher accuracy as $50\%$ so they are even worse or equally bad as deciding randomly if one stock goes up or down on the next day.
